该文章主要翻译自：

[High Volume Incoming Connections](https://levelup.gitconnected.com/linux-kernel-tuning-for-high-performance-networking-high-volume-incoming-connections-196e863d458a)。

#### Tcp Receive Queue

> The received frames will be stored in this queue after taking them from the ring buffer on the NIC. 

可以简单理解为网卡收到的报文都会放入该队列中，等待内核处理。

如果网卡收包的速率大于内核处理的速度，导致该队列对塞满，就会丢弃后面的包。

可以通过调节 `net.core.netdev_max_backlog` 来增大队列长度。

#### Tcp Backlog Queue

也可以称为 SYN Queue （半连接队列）。

内核会对从 Tcp Receive Queue 中收到的 SYN 包建立连接并放入 SYN Backlog Queue。该连接会被标识为 “SYN_RECV” 同时会发送 SYN+ACK 给客户端。当收到 ACK 时，连接状态会变为 “ESTABLISHED”并被放入 TCP Accept Queue。

该队列的长度通过参数 `net.ipv4.tcp_max_syn_backlog` 来调节。

**注意**：如果开启了 SYN cookies，SYN backlog 将被忽略。关于 SYN cookies 的内容，参考：[深入浅出TCP中的SYN-Cookies](https://segmentfault.com/a/1190000019292140)。

此外，还有 “SYN+ACK Retries” 和 “SYN Retries” 的概念，此处不展开。

可以参考图：

![img](http://poettian-md-doc.oss-cn-beijing.aliyuncs.com/paper/20220501/dcZ30f.png)

具体细节可以参考：[How TCP backlog works in Linux](https://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html)。

#### TCP Accept Queue

也叫全连接队列。

每次应用调用 accept() 函数会移除队列头的连接。如果队列为空，accept() 通常会阻塞。

该队列的长度取决于（两者的最小值）：

1）the backlog parameter on the TCP listen() call

2）a kernel limit maximum from the kernel setting `net.core.somaxconn`

#### File Descriptors

理解文件描述符：[文件描述符与socket连接](https://www.cnblogs.com/DengGao/p/file_symbol.html)。

##### 系统限制：

最大可用文件描述符数量由内核参数 `fs.file-max` 控制。

##### 用户限制：

用户最大可用文件描述符数量，使用命令 `ulimit -n -H` 查看硬限制，`ulimit -n -S` 查看软限制。

在 `/etc/security/limits.conf` 中修改 “nofile”。

> 软限制起实际限制作用，但不能超过硬限制（除非有root权限）。
> 普通用户可以在硬限制范围内，更改自己的软限制
> 普通用户都可以缩小硬限制,但不能扩大硬限制，而root缩小扩大都可以。

此外，如果程序在 systemd 下运行，参考“LimitNOFILE”（比如： `systemctl show nginx | grep LimitNOFILE` ）。

#### Threads Limit

同上面的 File Descriptors：

##### 系统限制：

`kernel.threads-max`

##### 用户限制：

使用 `ulimit -u` 查看。

在 limits.conf 中对应为 “nproc”，在 systemd 下对应为 “LimitNPROC”。

#### TIME_WAIT

> Under high volume burst traffic, proxy connections stuck in “TIME_WAIT” can add up tying up many resources during the close connection handshake. This state indicates the client has received a final **FIN** packet from the server (or upstream worker) and is being kept around to any delayed in-flight packets to be properly handled. The time the connection exists in “TIME_WAIT” by default is 2 x MSL (Maximum Segment Length), which is 2 x 60s. In many cases, this is normal and expected behavior and the default of 120s is acceptable. However, when the volume of connections in the “TIME_WAIT” state is high, this can cause the application to run out of ephemeral ports to connect to a client socket. In this case, let these time out faster by reducing the **FIN** timeout.
>
> The kernel setting that controls this timeout is `net.ipv4.tcp_fin_timeout` and a good setting for a high performance server is between 5 and 7 seconds.

#### 优化建议

> The **receive queue** should be sized to handle as many packets as linux can process off of the NIC without causing dropped packets, including some small buffer in case spikes are a bit higher than expected. The softnet_stat file should be monitored for dropped packets to discover the correct value. A good rule of thumb is to use the value set for tcp_max_syn_backlog to allow for at least as many **SYN** packets that can be processed to create half-open connections. Remember, this is the number of packets each CPU can have in its receive buffer, so divide the total desired by the number of CPUs to be conservative.
>
> The **SYN backlog queue** should be sized to allow for a large number of half-open connections on a high performance server to handle bursts of occasional spike traffic. A good rule of thumb is to set this at least to the highest number of established connections a listener can have in the accept queue, but no higher than twice the number of established connections a listener can have. It is also recommended to turn off SYN cookie protection on these systems to avoid data loss on high burst initial connections from legitimate clients.
>
> The **accept queue** should be sized to allow for holding a volume of established connections waiting to be processed as a temporary buffer during periods of high burst traffic. A good rule of thumb is to set this between 20–25% of the number of worker threads.

```bash
# /etc/sysctl.d/00-network.conf
# Receive Queue Size per CPU Core, number of packets
# Example server: 8 cores
net.core.netdev_max_backlog = 4096
# SYN Backlog Queue, number of half-open connections
net.ipv4.tcp_max_syn_backlog = 32768
# Accept Queue Limit, maximum number of established
# connections waiting for accept() per listener.
net.core.somaxconn = 65535
# Maximum number of SYN and SYN+ACK retries before
# packet expires.
net.ipv4.tcp_syn_retries = 1
net.ipv4.tcp_synack_retries = 1
# Timeout in seconds to close client connections in
# TIME_WAIT after receiving FIN packet.
net.ipv4.tcp_fin_timeout = 5
# Disable SYN cookie flood protection
net.ipv4.tcp_syncookies = 0
# Maximum number of threads system can have, total.
# Commented, may not be needed. See user limits.
#kernel.threads-max = 3261780
# Maximum number of file descriptors system can have, total.
# Commented, may not be needed. See user limits.
#fs.file-max = 3261780
```

```bash
# /etc/security/limits.d/nginx.conf
nginx soft nofile 64000
nginx hard nofile 64000
nginx soft nproc 64000
nginx hard nproc 64000
```

#### 参考文章：

[Linux 网络调优：内核网络栈参数篇](https://www.starduster.me/2020/03/02/linux-network-tuning-kernel-parameter/)

[理解 Linux backlog/somaxconn 内核参数](https://jaminzhang.github.io/linux/understand-Linux-backlog-and-somaxconn-kernel-arguments/)

[再聊 TCP backlog - 有点存疑](https://juejin.cn/post/6844904071367753736)

[Linux实例常用内核网络参数介绍与常见问题处理](https://help.aliyun.com/document_detail/41334.html#h3-linux-fin_wait2-tcp-)